{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 Project - Starter Notebook\n",
    "\n",
    "This notebook has been provided to you so that you can make use of the following starter code to help with the trickier parts of preprocessing the Zillow dataset. \n",
    "\n",
    "The notebook contains a rough outline the general order you'll likely want to take in this project. You'll notice that most of the areas are left blank. This is so that it's more obvious exactly when you should make use of the starter code provided for preprocessing. \n",
    "\n",
    "**_NOTE:_** The number of empty cells are not meant to infer how much or how little code should be involved in any given step--we've just provided a few for your convenience. Add, delete, and change things around in this notebook as needed!\n",
    "\n",
    "# Some Notes Before Starting\n",
    "\n",
    "This project will be one of the more challenging projects you complete in this program. This is because working with Time Series data is a bit different than working with regular datasets. In order to make this a bit less frustrating and help you understand what you need to do (and when you need to do it), we'll quickly review the dataset formats that you'll encounter in this project. \n",
    "\n",
    "## Wide Format vs Long Format\n",
    "\n",
    "If you take a look at the format of the data in `zillow_data.csv`, you'll notice that the actual Time Series values are stored as separate columns. Here's a sample: \n",
    "\n",
    "<img src='~/../images/df_head.png'>\n",
    "\n",
    "You'll notice that the first seven columns look like any other dataset you're used to working with. However, column 8 refers to the median housing sales values for April 1996, column 9 for May 1996, and so on. This This is called **_Wide Format_**, and it makes the dataframe intuitive and easy to read. However, there are problems with this format when it comes to actually learning from the data, because the data only makes sense if you know the name of the column that the data can be found it. Since column names are metadata, our algorithms will miss out on what dates each value is for. This means that before we pass this data to our ARIMA model, we'll need to reshape our dataset to **_Long Format_**. Reshaped into long format, the dataframe above would now look like:\n",
    "\n",
    "<img src='~/../images/melted1.png'>\n",
    "\n",
    "There are now many more rows in this dataset--one for each unique time and zipcode combination in the data! Once our dataset is in this format, we'll be able to train an ARIMA model on it. The method used to convert from Wide to Long is `pd.melt()`, and it is common to refer to our dataset as 'melted' after the transition to denote that it is in long format. \n",
    "\n",
    "# Helper Functions Provided\n",
    "\n",
    "Melting a dataset can be tricky if you've never done it before, so you'll see that we have provided a sample function, `melt_data()`, to help you with this step below. Also provided is:\n",
    "\n",
    "* `get_datetimes()`, a function to deal with converting the column values for datetimes as a pandas series of datetime objects\n",
    "* Some good parameters for matplotlib to help make your visualizations more readable. \n",
    "\n",
    "Good luck!\n",
    "\n",
    "\n",
    "# Step 1: Load the Data/Filtering for Chosen Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import helper_functions as hf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from matplotlib.pylab import rcParams\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wide = pd.read_csv('./zillow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_long = hf.melt_data_v2(data_wide)\n",
    "data_long['time'] = pd.to_datetime(data_long['time'], format='%Y-%m-%d')\n",
    "data_long['RegionName'] = data_long['RegionName'].astype('str')\n",
    "\n",
    "data_long.set_index(keys='time', inplace=True)\n",
    "data_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be looing at zip codes in Florida, specifically areas near Orlando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out areas within the state of Florida\n",
    "df_fl = data_long.loc[data_long.State=='FL']\n",
    "df_fl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl.info()\n",
    "df_fl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "rc('font', **font)\n",
    "\n",
    "# NOTE: if you visualizations are too cluttered to read, try calling 'plt.gcf().autofmt_xdate()'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the state array, there are 51 entries.\n",
    "data_long.State.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Looking at each state to see the total means over the years\n",
    "plt.figure(figsize=(21,13))\n",
    "for state in data_long.State.unique():\n",
    "    state_data = data_long.loc[(data_long.State==state), ['value']].resample('MS').sum()\n",
    "    plt.plot(state_data, label=state)\n",
    "    \n",
    "plt.title('Sum of monthly means of US states housing market')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to print out the counties for the state of Florida\n",
    "plt.figure(figsize=(21,13))\n",
    "for county in df_fl.CountyName.unique():\n",
    "    county_data = df_fl.loc[(df_fl.CountyName==county), ['value']].resample('MS').mean()\n",
    "    plt.plot(county_data, label=county)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida monthly means ovre the years\n",
    "florida_monthly = df_fl.groupby(pd.Grouper(freq='MS'))\n",
    "florida_monthly.value.mean().plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate out the areas that are part of Orange County, Florida BY YEAR 2011\n",
    "orange_county = df_fl.loc[(df_fl.CountyName=='Orange'), ['value']].resample('MS').mean()\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(orange_county)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# df_fl_2011.loc[(df_fl_2011.CountyName=='Orange'), ['value']].resample('MS').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Orange county's prices greatly resemble that of the entire state of Florida.\n",
    "\n",
    "Below, we will take a look at the zip codes that make up Orange county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zipcodes = df_fl.loc[(df_fl.CountyName=='Orange')].RegionName.unique()\n",
    "plt.figure(figsize=(15,5))\n",
    "for zip_code in zipcodes:\n",
    "    area = df_fl.loc[(df_fl.RegionName==zip_code), ['value']].resample('MS').mean()\n",
    "    plt.plot(area, label=zip_code)\n",
    "    plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our graph below, there appears to be some seasonality that is occurring within the housing market for orange county as time passes by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Looking at difference by year\n",
    "# Look at the distribution of the diffs and look at the one with the smallest standard deviation\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "orange_county_diff = orange_county.diff(periods=1)\n",
    "rcParams['figure.figsize'] = (12, 5)\n",
    "plt.plot(orange_county_diff)\n",
    "\n",
    "rcParams['figure.figsize'] = (12, 5)\n",
    "plot_acf(orange_county['value'], title='Orange County Auto Correlation');\n",
    "plot_pacf(orange_county['value'], title='Orange County Partial Auto Correlation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what we see in our partial correlation plot, there is a high negative correlation somewhere between 220 - 245 lags.\n",
    "\n",
    "This high negative appears at lag = 180 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Reshape from Wide to Long Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orange_county.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: ARIMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the ARIMA modeling, combinations for the model needs to be created.  \n",
    "Here, the parameters for all combinations of seasons are also added to our values for seasonal & non seasonal arima modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "## Generate combinations of SARIMA modeling with different seasonalities\n",
    "# pdqs = []    \n",
    "# for i in range(0,13):\n",
    "#     for x in pdq:\n",
    "#         pdqs.append((x[0], x[1], x[2], i))\n",
    "\n",
    "pdqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NEED TO TURN THIS INTO A FUNCTION THAT I CAN PASS\n",
    "## EACH ZIP CODE INTO AS A DATAFRAME, \n",
    "ans = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = hf.model_SARIMA(df=orange_county, order=comb, s_order=combs)\n",
    "            ans.append([comb, combs, mod.aic, mod.bic])\n",
    "            print('ARIMA {} x {} : AIC Calculated ={}, BIC Calculated ={}'.format(comb, combs, mod.aic, mod.bic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running all of the possible combinations through the seasonal ARIMA model, the results of each combination was stored in a dataframe, so that we can easily search for the optimum model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'aic', 'bic'])\n",
    "ans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the our best model has both the highest `aic`and `bic` scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df.loc[ans_df['aic'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df.loc[ans_df['bic'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the results from our SARIMA model, we will now take the results with best AIC and BIC and pass it into our Model to see how ti performed compared to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting ARIMA Time Series Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get the results of our best parameters for our ARIMA model ###\n",
    "order = ans_df.loc[ans_df['aic'].idxmin()]['pdq']\n",
    "s_order = ans_df.loc[ans_df['aic'].idxmin()]['pdqs']\n",
    "ARIMA_MODEL = hf.model_SARIMA(orange_county, order=order, s_order=s_order, print_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to take a look where some of the residuals are deviating from the standard deviation and attempt to create a batter model with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plottting residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hf.diagnostics_plot(model=ARIMA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the plot diagnostics, we see that our data is not normally distributed. From here, we will continue to further look into our model and attempt to improve the results by removing outliers and the residuals that are causing issues within our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using our model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orange_county.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf.one_step_ahead_forecast(df=orange_county, start_date='2014', end_date='2021', arima_model=ARIMA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic forecasting of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.dynamic_prediction(df=orange_county, start_date='2014', end_date='2021', arima_model=ARIMA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of our dynamic prediction, the housign prices are forecast to steadily increase into 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findng the best zip code within Orange County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best zip code within the given area, we wil use the following formula to calculate the return of investment:\n",
    "\n",
    "$$\\large R.O.I = \\frac{(GFI - CoI)}{CoI}$$\n",
    "\n",
    "- ROI = Return of Investment\n",
    "- GFI = Gain from Investment\n",
    "- CoI = Cost of Investment\n",
    "\n",
    "Our Cost of Ivestment will be the average of 2017, since we do not have a complete dataset for 2018\n",
    "\n",
    "\n",
    "To calculate GFI, we will take our cost of investment and subtract it from the average predicted means from 2018 to 2021.\n",
    "\n",
    "We will then use the formula above to calculate the return of investment for each zip code observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zipcodes of Orange County, Florida\n",
    "zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROI_list = []\n",
    "model_list = []\n",
    "\n",
    "## Loop to get each zip code and calculate return on ivestment ##\n",
    "for code in zipcodes:\n",
    "    zip_df = df_fl.loc[(df_fl.RegionName==code), ['value']].resample('MS').mean()\n",
    "    zip_model = model_SARIMA(zip_df, order=order, s_order=s_order)\n",
    "\n",
    "    pred = zip_model.get_prediction(start=pd.to_datetime('2014'), end=pd.to_datetime('2021'))\n",
    "\n",
    "    ## Define the initial cost of investment as of 2017 ##\n",
    "    cost_of_investment = zip_df['2017'].value.mean()\n",
    "\n",
    "    ## Calculate gain from investmnt from 2018 up to 2021 ##\n",
    "    gain_from_investment = pred.predicted_mean['2018':].mean()\n",
    "\n",
    "    ## calculate Return of Investment for the observed zip code\n",
    "    ROI = (gain_from_investment - cost_of_investment)/cost_of_investment\n",
    "    ROI_list.append(ROI)\n",
    "    model_list.append(zip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(data=list(zip(zipcodes, ROI_list, model_list)), columns=['zip_code','ROI','model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of our model, the top 5 zip codes to purchase a house from 2018 -2021 are as follows:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results.sort_values(by='ROI', ascending=False).drop('model', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results of our modeling and testing, the best zip code in Orange County to purchase a home is 32839.\n",
    "The return on investment for this home is listed at 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
